{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interoperability, or making things speak to each other\n",
    "## Or, turning everything into numbers\n",
    "\n",
    "In this studio, we're going to play around with some functions of interoperability - or, explore different data structures and how to make them connect up. \n",
    "\n",
    "I find this kind of data science really frustrating (even though I work in data visualization), because it is really unintiuitive. It requires a *weird* kind of imagining, so if you feel like you're losing your footing, don't worry, you're not alone. It's vertiginous!\n",
    "\n",
    "But, it means that we get to roll together some of the ideas we learnt about in our Tables studio, and our Algorithms studio, and prepare for our Images studio in a few weeks time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning the social into numbers\n",
    "\n",
    "Let's start by going back and getting some data from our last python studio.\n",
    "\n",
    "## Data from your Reddit API\n",
    "\n",
    "Sign in to reddit using Google Chrome in a separate tab.\n",
    "\n",
    "Then go to this page: https://www.reddit.com/prefs/apps\n",
    "\n",
    "You should already have an app. If you don't, click **create app**\n",
    "\n",
    "![create app](https://miro.medium.com/max/2400/1*I06ZUKgMjooh2hFopGrqjQ.png)\n",
    "\n",
    "In the form that will open, you should enter your name, description and uri. For the redirect uri you should choose http://localhost:8080\n",
    "\n",
    "![redirect uri](https://miro.medium.com/max/2400/1*SrohBPmEox1R9Qdp0K8Z6w.jpeg)\n",
    "\n",
    "Now, let's import our packages and set up our API connection. You need to fill out your own ID details!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "reddit = praw.Reddit(client_id=\"bst9HmJXayXZFw\",      # your client id\n",
    "                     client_secret=\"Iq1bwbLIaowB25CY-v6F1qPMLIBuIA\",  #your client secret\n",
    "                     user_agent=\"android:com.example.myredditapp:v1.2.3 (by u/Glass_Relationship_3)\", #user agent name\n",
    "                     username = \"Glass_Relationship_3\",     # your reddit username\n",
    "                     password = \"studios9876\")     # your reddit password\n",
    "\n",
    "print(reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's scrape our subreddit. In the `sub` section you can choose your subreddit, and then use `query` to run a search term.  \n",
    "\n",
    "At the end we'll convert it into a panda data frame called \"post_data\" which we will use for later gymnastics, and save it to CSV for good measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = ['berkeley'] # your subreddit\n",
    "\n",
    "for s in sub:\n",
    "    \n",
    "    subreddit = reddit.subreddit(s)\n",
    "    query = ['I']\n",
    " \n",
    "    for item in sub:\n",
    "        posts = {\n",
    "            \"title\" : [],   #title of the post\n",
    "            \"score\" : [],   # score of the post\n",
    "            \"id\" : [],      # unique id of the post\n",
    "            \"url\" : [],     #url of the post\n",
    "            \"comms_num\": [],   #the number of comments on the post\n",
    "            \"created\" : [],  #timestamp of the post\n",
    "            \"upvote_ratio\" : [],         # the description of post\n",
    "            \"body\" : [] #the body of the post\n",
    "        }\n",
    "        for submission in subreddit.search(query,sort = \"top\",limit = 1000): #max 1k\n",
    "            posts[\"title\"].append(submission.title)\n",
    "            posts[\"score\"].append(submission.score)\n",
    "            posts[\"id\"].append(submission.id)\n",
    "            posts[\"url\"].append(submission.url)\n",
    "            posts[\"comms_num\"].append(submission.num_comments)\n",
    "            posts[\"created\"].append(submission.created_utc)\n",
    "            posts[\"upvote_ratio\"].append(submission.upvote_ratio)\n",
    "            posts[\"body\"].append(submission.selftext)\n",
    "        \n",
    "\n",
    "        post_data = pd.DataFrame(posts)\n",
    "        post_data.to_csv(s+\"_\"+ item +\"subreddit.csv\")\n",
    "\n",
    "print(subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more info on the parameters you can request for a submission, see: http://lira.no-ip.org:8080/doc/praw-doc/html/code_overview/models/submission.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding numbers in data\n",
    "\n",
    "This next section, we're going to get used to different computational types and how they work together.\n",
    "\n",
    "Let's see what our post_data from Reddit looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different data types have different properties which allow them to do things, or not do things. For instance, you can't plot a character on a graph.\n",
    "\n",
    "In Python, these are the main data types (thanks to Shawn Ren for the graph):\n",
    "![](https://miro.medium.com/max/700/1*QfI8H_8HplGa1v9IrrWjBA.png) \n",
    "\n",
    "So, let's check out the data types of our `post_data` data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(post_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're seeing a lot of Python/Panda objects (because this is a dataframe, and which we will need to convert to use), but also some integers and floating points, which are numeric forms. This is awesome! \n",
    "\n",
    "So, let's try plotting some data using matplotlib's pyplot. Most digital images are Cartesian (like maps!), meaning that they work on an x,y axis, where each pixel is assigned an x,y coordinate. This coordinate system, called algebraic geometry, combines spatial measurement forms with numeric forms.\n",
    "\n",
    "![](https://images.deepai.org/glossary-terms/7d0273fdc6cc42aca2fbdd72b61a4499/cartesian.png)\n",
    "\n",
    "So, you can set any of the int64 or float74 values against each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# scatter the comments against the likes\n",
    "ax.scatter(post_data['upvote_ratio'], post_data['comms_num']) #format (dataframe1(column), dataframe2(column))\n",
    "# set a title and labels\n",
    "ax.set_title('Number of Comments vs Upvote Ratio')\n",
    "ax.set_xlabel('No of Upvotes to Downvotes')\n",
    "ax.set_ylabel('number of comments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, cool. But what about the time of the post. Take a look at the \"created\" column - this is a time stamp in Unix time, which is a universal time that is free from timezones:\n",
    "\n",
    "> Unix time (a.k.a. POSIX time or Epoch time) is a system for describing instants in time, defined as the number of seconds that have elapsed since 00:00:00 Coordinated Universal Time (UTC), Thursday, 1 January 1970, not counting leap seconds. It is used widely in Unix-like and many other operating systems and file formats. Due to its handling of leap seconds, it is neither a linear representation of time nor a true representation of UTC. \n",
    "\n",
    "We're going to need to bring that into something readable to humans! So, let's convert it, and make sure that it's in a `datetime` format and that it looks about right to the human eye:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_data[\"created\"]= pd.to_datetime(post_data[\"created\"], yearfirst=True, unit=\"s\")\n",
    "print(post_data.dtypes)\n",
    "post_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the date compared to the number of comments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# scatter the comments against the likes\n",
    "ax.scatter(post_data['created'], post_data['comms_num'])\n",
    "# set a title and labels\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Number of Comments on Posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we've been using the useful and classic `matplotlib` to do our graphics. But it's not really the best. Let's try another and see if we can get some more information. Let's use `plotly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly==4.14.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(post_data, x=\"created\", y=\"upvote_ratio\", size=\"comms_num\", color=\"score\", hover_name=\"title\", size_max=60)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not going to bore you with more graphs - but when you're feeling up to it, feel free to take a look at the different kinds of charts you can make and have a play around - you could even combine several reddit datasets!\n",
    "\n",
    "https://plotly.com/python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning our bodies into numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's turn to something a little more complicated, with some reflections on Wernimont's piece on the Quantified Self and explore some of the ways in which our bodies are made data. \n",
    "\n",
    "I've located and exported my own (seriously incomplete, and didn't even realise I had authorised it) health data from my iPhone's Health App for a laugh. \n",
    "\n",
    "When downloaded, this comes in a .zip format. When expanded, you get two files - `export.xml` is the one that we want. \n",
    "\n",
    "XML, like geojson is good format for holding together different types of data in the same document (like we learned with geojson). But it's not super useful for python, so we're going to run the `apple-health-data-parser` created by Nicholas Radcliffe to \"parse\" or separate out the data into different CSV files. Then we can have a little look at it more closely.\n",
    "\n",
    "Normally, we would run a .py file using the command line (like terminal), but Jupyter is friendly, and actually lets us run .py files like a command line from inside the notebook! So, making sure that the following are in the same folder (which they will be if you have downloaded this from github) - `Interoperability_Studio.ipynb`, `apple-heath-data-parser.py` and `export.xml`, let's try to do some parsing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %run -i 'apple-health-data-parser' 'export.xml' \n",
    "%run -i 'apple-health-data-parser' 'export.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Looks like like Apple has been secretly collecting four kinds of my data: flights of stairs climbed, how often and loudly I use my headphones, my step count and how far I walk. Let's explore some of this data.\n",
    "\n",
    "We start by installing (if we haven't already) 3 libraries: `numpy` (or nummber python, num-py), `pandas` (our much loved data format), and `glob`, which helps us find data paths on our computers, the `pytz` time zone calculator, `pyplot` for making graphs, and `datetime`, which does as it says. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import date, datetime, timedelta as td\n",
    "import pytz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's see what this data is all about. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "steps = pd.read_csv(\"StepCount.csv\") #use pandas (pd) to read the csv file\n",
    "steps.head() #have a look at the top row of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check out what kind of data we're working with here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(steps.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of objects, again, and some messy time formats too. Let's clean up. We need to start with date-time - the data crosses a few timezones, I think, but I want to bring it into the one I'm in now - America/Los_Angeles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to convert UTC to LA time zone and extract date/time elements\n",
    "convert_tz = lambda x: x.to_pydatetime().replace(tzinfo=pytz.utc).astimezone(pytz.timezone('America/Los_Angeles'))\n",
    "get_year = lambda x: convert_tz(x).year\n",
    "get_month = lambda x: '{}-{:02}'.format(convert_tz(x).year, convert_tz(x).month) #inefficient\n",
    "get_date = lambda x: '{}-{:02}-{:02}'.format(convert_tz(x).year, convert_tz(x).month, convert_tz(x).day) #inefficient\n",
    "get_day = lambda x: convert_tz(x).day\n",
    "get_hour = lambda x: convert_tz(x).hour\n",
    "get_minute = lambda x: convert_tz(x).minute\n",
    "get_day_of_week = lambda x: convert_tz(x).weekday()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's \"parse\" (or separate) out the different time sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse out date and time elements as LA time\n",
    "steps['startDate'] = pd.to_datetime(steps['startDate'])\n",
    "steps['year'] = steps['startDate'].map(get_year)\n",
    "steps['month'] = steps['startDate'].map(get_month)\n",
    "steps['date'] = steps['startDate'].map(get_date)\n",
    "steps['day'] = steps['startDate'].map(get_day)\n",
    "steps['hour'] = steps['startDate'].map(get_hour)\n",
    "steps['dow'] = steps['startDate'].map(get_day_of_week)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check it's lookin' good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coolios - as you can see above, EVERYTHING IS NUMBERS. SEPARATE CATEGORISED NUMBERS. What are those categories, you ask?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create some groups for each date, to see how many each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_by_date = steps.groupby(['date'])['value'].sum().reset_index(name='Steps')\n",
    "steps_by_date.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's save it to CSV for good measure, and so we can start visualising!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_by_date.to_csv(\"steps_per_day.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to turn numbers back into images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_by_date['RollingMeanSteps'] = steps_by_date.Steps.rolling(window=10, center=True).mean()\n",
    "steps_by_date.plot(x='date', y='RollingMeanSteps', title= 'Daily step counts rolling mean over 10 days', figsize=[10, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about weekday? Let's regroup our CSV and see what we find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regroup\n",
    "steps_by_date['date'] = pd.to_datetime(steps_by_date['date'])\n",
    "steps_by_date['dow'] = steps_by_date['date'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "\n",
    "data = steps_by_date.groupby(['dow'])['Steps'].mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[10, 6])\n",
    "ax = data.plot(kind='bar', x='day_of_week')\n",
    "\n",
    "n_groups = len(data)\n",
    "index = np.arange(n_groups)\n",
    "opacity = 0.75\n",
    "\n",
    "#fig, ax = plt.subplots(figsize=[10, 6])\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "plt.suptitle('Average Steps by Day of the Week', fontsize=16)\n",
    "dow_labels = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "plt.xticks(index, dow_labels, rotation=45)\n",
    "plt.xlabel('Day of Week', fontsize=12, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about hours (bearing in mind time zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_steps = steps.groupby(['hour'])['value'].sum().reset_index(name='Steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = hour_steps.Steps.plot(kind='line', figsize=[10, 5], linewidth=4, alpha=1, marker='o', color='#6684c1', \n",
    "                      markeredgecolor='#6684c1', markerfacecolor='w', markersize=8, markeredgewidth=2)\n",
    "\n",
    "xlabels = hour_steps.index.map(lambda x: '{:02}:00'.format(x))\n",
    "ax.set_xticks(range(len(xlabels)))\n",
    "ax.set_xticklabels(xlabels, rotation=45, rotation_mode='anchor', ha='right')\n",
    "\n",
    "# ax.set_xlim((hour_steps.index[0], hour_steps.index[-1]))\n",
    "\n",
    "ax.yaxis.grid(True)\n",
    "# ax.set_ylim((0, 1300))\n",
    "ax.set_ylabel('Steps')\n",
    "ax.set_xlabel('')\n",
    "ax.set_title('Steps by hour the day')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine the numeric representation of my lived mobilities. What about flights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = pd.read_csv(\"FlightsClimbed.csv\") #use pandas (pd) to read the csv file\n",
    "flights.head() #have a look at the top row of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's parse it out again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse out date and time elements as LA time\n",
    "flights['startDate'] = pd.to_datetime(flights['startDate'])\n",
    "flights['year'] = flights['startDate'].map(get_year)\n",
    "flights['month'] = flights['startDate'].map(get_month)\n",
    "flights['date'] = flights['startDate'].map(get_date)\n",
    "flights['day'] = flights['startDate'].map(get_day)\n",
    "flights['hour'] = flights['startDate'].map(get_hour)\n",
    "flights['dow'] = flights['startDate'].map(get_day_of_week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And group it into dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_by_date = flights.groupby(['date'])['value'].sum().reset_index(name='Flights')\n",
    "flights_by_date.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_by_date.to_csv(\"flights_by_date.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_by_date['date'] = pd.to_datetime(flights_by_date['date'])\n",
    "flights_by_date['dow'] = flights_by_date['date'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "\n",
    "data = flights_by_date.groupby(['dow'])['Flights'].mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[10, 6])\n",
    "ax = data.plot(kind='bar', x='day_of_week')\n",
    "\n",
    "n_groups = len(data)\n",
    "index = np.arange(n_groups)\n",
    "opacity = 0.75\n",
    "\n",
    "#fig, ax = plt.subplots(figsize=[10, 6])\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "plt.suptitle('Average Flights by Day of the Week', fontsize=16)\n",
    "dow_labels = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "plt.xticks(index, dow_labels, rotation=45)\n",
    "plt.xlabel('Day of Week', fontsize=12, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attaching numbers to numbers\n",
    "\n",
    "To be totally ridiculous, let's compare how many steps I take per day of the week, compared to how many comments on your chosen subreddit.\n",
    "\n",
    "First, we need to parse out (again) the time/date data. Then, it's just like above, using \"groupby\", while paying attention to the column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse out date and time elements as LA time\n",
    "post_data['created'] = pd.to_datetime(post_data['created'])\n",
    "post_data['year'] = post_data['created'].map(get_year)\n",
    "post_data['month'] = post_data['created'].map(get_month)\n",
    "post_data['date'] = post_data['created'].map(get_date)\n",
    "post_data['day'] = post_data['created'].map(get_day)\n",
    "post_data['hour'] = post_data['created'].map(get_hour)\n",
    "post_data['dow'] = post_data['created'].map(get_day_of_week)\n",
    "\n",
    "post_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = flights_by_date.groupby(['dow'])['Flights'].sum()\n",
    "s_df = steps_by_date.groupby(['dow'])['Steps'].median()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[10, 6])\n",
    "f_ax = f_df.plot(kind='line', x='day_of_week')\n",
    "s_ax = s_df.plot(kind='line', x='day_of_week')\n",
    "\n",
    "#fig, ax = plt.subplots(figsize=[10, 6])\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "plt.suptitle('Steps VS Reddit Comments', fontsize=16)\n",
    "dow_labels = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "plt.xticks(index, dow_labels, rotation=45)\n",
    "plt.xlabel('Day of Week', fontsize=12, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a mess around in your own time - compare, means to medians, and ask your friends in data science what it's all about, because honestly, it's just a strange kind of magic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning words into numbers\n",
    "\n",
    "Turning back to our subreddit, and channelling cultural analytics, let's look a little more closely at some text analysis and see what we can do!\n",
    "\n",
    "Text works as a `str` or string: \n",
    "\n",
    "![](https://csharpcorner.azureedge.net/article/learn-about-strings-in-python/Images/Capture.PNG)\n",
    "\n",
    "A word is a string of individual letters, a sentence is a string of words!\n",
    "\n",
    "(Strings are used a lot in the Digital Humanities and Text Processing - I'm a geographer, and still learning about strings, so bear with me!)\n",
    "\n",
    "Let's start by grabbing a cell with an object from our `post_data` dataset. With a `pandas` data frame, everything works on a gridded position as well! You can use `iloc` (or location by position) to find particular cells. Let's start with row number 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "post_data.iloc[3] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you count down the list, body is number \"7\", so let's add that to get the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_data.iloc[3,7] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's convert it from a panda object to a string, and give it a name, so we can do some analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = str(post_data.iloc[3,7])\n",
    "print(cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count how many characters are in the string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cell) #len = length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or what the 'n' letter of the string is (in the below example, 45th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell[45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting words\n",
    "\n",
    "If we wanted to be braver, we could even try to count the most common words all the posts in the \"title\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(\" \".join(post_data[\"title\"]).split()).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are many \"to\", \"the\", \"of\" .... These are called \"stopwords\". Let's create a new column with all the stopwords deleted so we can count again. \n",
    "\n",
    "To do this we import an nltk dictionary which has a list of words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we delete the stopwords from the title column and make a new column without the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_data['title_without_stopwords'] = post_data['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "post_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And try again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(\" \".join(post_data[\"title_without_stopwords\"]).split()).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! \n",
    "\n",
    "(as a bonus, you could turn this into a data frame if you wanted, and plot it as well! - though it's not a super interesting graph!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "words_num = Counter(\" \".join(post_data[\"title_without_stopwords\"]).split()).most_common(20)\n",
    "words_num_df = pd.DataFrame(words_num,columns=['word','count'])\n",
    "fig = px.scatter(words_num_df, x=\"word\", y=\"count\", hover_name=\"word\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning sound into numbers\n",
    "\n",
    "Okay, let's try some data that we don't necessarily think of as numeric: sound.\n",
    "\n",
    "Let's import some libraries to help us out with sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pydub\n",
    "! pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import those libraries and read our file. We're directly reference the `sound_sample.wav` that is in your downloaded folder. And let's print the rate and the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#required libraries\n",
    "import scipy.io.wavfile\n",
    "import pydub\n",
    "\n",
    "rate,audData=scipy.io.wavfile.read(\"sound_sample.wav\")\n",
    "\n",
    "print(rate)\n",
    "print(audData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the wavefile.read are the sampling rate on the track, and the audio wave data. The sampling rate represents the number of data points sampled per second in the audio file. In this case 44100 pieces of information per second make up the audio wave. This is a very common rate. The higher the rate, the better quality the audio.\n",
    "\n",
    "Let's take a shape of the audio data a second of audio data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wav length\n",
    "audData.shape[0] / rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the shape of the audio data it has ONE array, so it's a mono channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audData.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored as int16. This is the size of the data stored in each datapoint. Common storage formats are 8, 16, 32. Again the higher this is the better the audio quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in the data represent the amplitude of the wave (or the loudness of the audio). The energy of the audio can be described by the sum of the absolute amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Energy of music\n",
    "np.sum(audData.astype(float)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will depend on the length of the audio, the sample rate and the volume of the audio. A better metric is power, which is energy per second..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#power - energy per unit of time\n",
    "1.0/(2*(audData.size)+1)*np.sum(audData.astype(float)**2)/rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the amplitude of the track over time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#create a time variable in seconds\n",
    "time = np.arange(0, float(audData.shape[0]), 1) / rate\n",
    "\n",
    "#plot amplitude (or loudness) over time\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.plot(time, audData, linewidth=0.01, alpha=1, color='#00ff00')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common way to analyse audio is to create a spectogram. Audio spectograms are heat maps that show the frequencies of the sound in Hertz (Hz), the volume of the sound in Decibels (dB), against time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2, figsize=(8,6))\n",
    "plt.subplot(211)\n",
    "Pxx, freqs, bins, im = plt.specgram(audData, Fs=rate, NFFT=1024, cmap=plt.get_cmap('viridis'))\n",
    "cbar=plt.colorbar(im)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "cbar.set_label('Intensity dB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result allows us to pick out a certain frequency and examine it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(freqs==10034.47265625)\n",
    "MHZ10=Pxx[233,:]\n",
    "plt.plot(bins, MHZ10, color='#ff7f00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that's it for sound!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning images into grids into numbers\n",
    "\n",
    "In the final section of thsi studio, we're going to use a mixture of `matplotlib` and another library `imageio` to examine how images work as computational data (and how they're all also secretly grids and numbers). \n",
    "\n",
    "First, let's import `imageio` (`matplotlib` is already imported above), and drag in an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "#replace the link with the link to an image of your choice\n",
    "pic = imageio.imread(\"https://media.npr.org/assets/img/2017/04/25/istock-115796521-fcf434f36d3d0865301cdcb9c996cfd80578ca99-s800-c85.jpg\")\n",
    "plt.figure(figsize = (15,15))\n",
    "\n",
    "plt.imshow(pic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All digital images look like this (thanks Stanford for the image): \n",
    "\n",
    "![](https://web.stanford.edu/class/cs101/image-diagram1.png)\n",
    "\n",
    "Just like your graphs above, they have an `x` and `y` axis.\n",
    "\n",
    "Each pixel is made up of three values: red (r), green (g) and blue (b):\n",
    "\n",
    "![](https://web.stanford.edu/class/cs101/image-diagram2.png)\n",
    "\n",
    "We will investigate this a little more in our image workshop, but for now, this provides us two ways of classifying (and so, searching through) the enormous data set that is an image: colour, and position. \n",
    "\n",
    "First, let's check that your image is in 3 dimensions (or RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dimension of Image {}'.format(pic.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find the RGB value of a single pixel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = pic[100, 50]\n",
    "print(rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we split the layers so each image just shows the red, green and blue values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #thanks to Yassine Hamdaoui for the code\n",
    " \n",
    "fig, ax = plt.subplots(nrows = 1, ncols=3, figsize=(15,5))  \n",
    "for c, ax in zip(range(3), ax):     \n",
    "     # create zero matrix        \n",
    "     split_img = np.zeros(pic.shape, dtype=\"uint8\") \n",
    "     # 'dtype' by default: 'numpy.float64'  # assing each channel      \n",
    "     split_img[ :, :, c] = pic[ :, :, c] # display each channel     \n",
    "     ax.imshow(split_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we change the r value of the rows 50 to 150 to the full 255 intensity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "pic[50:150 , : , 0] = 255 # full intensity to those pixel's R channel \n",
    "plt.figure( figsize = (5,5)) \n",
    "plt.imshow(pic) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's just highlight only pixel values that are higher than 180 in the r channel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = imageio.imread(\"https://media.npr.org/assets/img/2017/04/25/istock-115796521-fcf434f36d3d0865301cdcb9c996cfd80578ca99-s800-c85.jpg\")\n",
    "red_mask = pic[:, :, 0] < 180\n",
    "pic[red_mask] = 0\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(pic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for today! Don't forget to post your graph or image in the #studios slack channel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
